# Installation Guide

## Table of Contents
- [Step 1: Create Conda Environment](#step-1-create-conda-environment)
- [Step 2: Install FiVE-Bench and Dependencies](#step-2-install-five-bench-and-dependencies)
  - [Clone FiVE-Bench Repository](#clone-five-bench-repository)
  - [Install Co-Tracker and IQA Repos](#install-co-tracker-and-iqa-repos)
- [Step 3: Run FiVE-Bench Evaluation](#step-3-run-five-bench-evaluation)
  - [Evaluation Example: Wan-Edit](#evaluation-example-wan-edit)
  - [Evaluate Your Own Method](#evaluate-your-own-method)



---
## Step 1: Create Conda Environment

```bash
conda create -n five-bench python=3.11 -y
conda activate five-bench
conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia
```

---

## Step 2: Install FiVE-Bench and Dependencies

â­ After installation, your directory structure should look like this:

```
ğŸ“ /path/to/code
â”œâ”€â”€ ğŸ“ co-tracker
â”œâ”€â”€ ğŸ“ FiVE-Bench
â”œâ”€â”€ ğŸ“ IQA-PyTorch
```
Make sure all dependencies for each subproject are installed accordingly.

> âš ï¸ **NOTE:** Replace `/path/to/code` in the [`./config.yaml`](./config.yaml) file with the actual path to your ***code*** directory.

### â¬‡ï¸ Install Co-Tracker and IQA Repos
- Motion Fidelity Score (MFS) @ Co-Tracker: To evaluate temporal consistency using MFS, install [Co-Tracker](https://github.com/facebookresearch/co-tracker) in the following path: `./code/co-tracker`.
    ```bash
    cd ./code
    git clone https://github.com/facebookresearch/co-tracker
    cd co-tracker
    pip install -e .
    pip install matplotlib flow_vis tqdm tensorboard


    mkdir -p checkpoints
    cd checkpoints
    # download the offline (single window) model
    wget https://huggingface.co/facebook/cotracker3/resolve/main/scaled_offline.pth
    cd ..
    ```


- Image Quality Assessment (IQA) @ NIQE: To evaluate image quality with NIQE, install [IQA-PyTorch](https://github.com/chaofengc/IQA-PyTorch) under `./code/IQA-PyTorch`.
Then, replace the default `inference_iqa.py` with the version provided in our repo at [`./files/inference_iqa.py`](./files/inference_iqa.py).

    ```bash
    # Install with pip
    pip install pyiqa

    # Install latest github version
    pip uninstall pyiqa # if have older version installed already 
    pip install git+https://github.com/chaofengc/IQA-PyTorch.git

    # Install with git clone
    cd ./code
    git clone https://github.com/chaofengc/IQA-PyTorch.git
    cd IQA-PyTorch
    # pip install -r requirements.txt
    python setup.py develop
    ```

    ğŸ’¡ Donâ€™t forget to replace `inference_iqa.py`:
    ```bash
    cp ../../files/inference_iqa.py ./inference_iqa.py
    ```

### â¬‡ï¸ Clone FiVE-Bench Repository
Download dataset and install the evaluation code

```bash
cd ./code
# evaluation code
git clone https://github.com/minghanli/FiVE-Bench.git
pip install -r requirements.txt

# FiVE-Bench dataset 
cd ./FiVE-Bench
git clone https://huggingface.co/datasets/LIMinghan/FiVE-Fine-Grained-Video-Editing-Benchmark
mv FiVE-Fine-Grained-Video-Editing-Benchmark data
unzip bmasks.zip images.zip videos.zip
```

The data structure should looks like:

  ```json
  ğŸ“ data
  â”œâ”€â”€ ğŸ“ assets/
  â”œâ”€â”€ ğŸ“ edit_prompt/
  â”‚   â”œâ”€â”€ ğŸ“„ edit1_FiVE.json
  â”‚   â”œâ”€â”€ ğŸ“„ edit2_FiVE.json
  â”‚   â”œâ”€â”€ ğŸ“„ edit3_FiVE.json
  â”‚   â”œâ”€â”€ ğŸ“„ edit4_FiVE.json
  â”‚   â”œâ”€â”€ ğŸ“„ edit5_FiVE.json
  â”‚   â””â”€â”€ ğŸ“„ edit6_FiVE.json
  â”œâ”€â”€ ğŸ“„ README.md
  â”œâ”€â”€ ğŸ“¦ bmasks.zip 
  â”œâ”€â”€ ğŸ“ bmasks 
  â”‚   â”œâ”€â”€ ğŸ“ 0001_bus
  â”‚       â”œâ”€â”€ ğŸ–¼ï¸ 00001.jpg
  â”‚       â”œâ”€â”€ ğŸ–¼ï¸ 00002.jpg
  â”‚       â”œâ”€â”€ ğŸ–¼ï¸ ...
  â”‚   â”œâ”€â”€ ğŸ“ ...
  â”œâ”€â”€ ğŸ“¦ images.zip 
  â”œâ”€â”€ ğŸ“ images
  â”‚   â”œâ”€â”€ ğŸ“ 0001_bus
  â”‚       â”œâ”€â”€ ğŸ–¼ï¸ 00001.jpg
  â”‚       â”œâ”€â”€ ğŸ–¼ï¸ 00002.jpg
  â”‚       â”œâ”€â”€ ğŸ–¼ï¸ ...
  â”‚   â”œâ”€â”€ ğŸ“ ...
  â”œâ”€â”€ ğŸ“¦ videos.zip 
  â”œâ”€â”€ ğŸ“ videos
  â”‚   â”œâ”€â”€ ğŸï¸ 0001_bus.mp4
  â”‚   â”œâ”€â”€ ğŸï¸ 0002_girl-dog.mp4
  â”‚   â”œâ”€â”€ ğŸï¸ ...
  ```

---

## Step 3: Run FiVE-Bench Evaluation

### ğŸ¯ Evaluation Example: Wan-Edit
As an example, you can run evaluation using the **Wan-Edit** results. We use the edited results in `./data/results/Wan-Edit` with prompts from `./data/edit_prompt/edit5_FiVE.json`. Then run:

```bash
cd FiVE-Bench
sh scripts/eval_FiVE.sh --annotation_mapping_files "data/edit_prompt/edit5_FiVE.json" --tgt_methods "8_Wan_Edit" 
```

The evaluation result files should be found in:


```
ğŸ“ outputs
â”œâ”€â”€ ğŸ“„ edit5_FiVE_evaluation_result_frame_stride8.csv
â”œâ”€â”€ ğŸ“„ edit5_FiVE_evaluation_result_frame_stride8_avg.csv
```

### ğŸ¯ Evaluate Your Own Method
If you want to evaluate **your own method**, you can modify the following parameters in [`config.yaml`](./config.yaml) and [`evaluation/evaluate.py`](evaluation/evaluate.py):

- `root_tgt_video_folder`: the root directory where your edited videos are stored  
- `all_tgt_video_folders`: a list of subfolders corresponding to your method(s)

Updating these paths allows the evaluation script to locate and assess your results accordingly.

---